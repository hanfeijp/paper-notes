# [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
* Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio *


TLDR: The use of two networks, a generator that tries to create examples similar to the data and a discriminator that tries to tell if an example is fake. This leads to a minimax objective, where one discriminator is trying to maximize it's probability of detection and the generator is trying to minimize it. 

## Method

### Model

#### Generator:

Let `G` be a multilayer perceptron. Given some `z` drawn from a noise distribution `z~p_z`, `G(z)` aims to recreate the data distribution `p_d`.

#### Discriminator:

Let 'D' be a multilayer perceptron. Given an example `x`, `D(x)` represents the probability that `x` was drawn from the data distribution `p_d`.

#### Objective

The discriminator wants to minimize `D(G(z))` (alternatively maximize `1-D(G(z))`) and maximize `D(x)` where `x~p_d`. The generator wants to do the opposite, leading to the objective:

`min_G max_D log D(x) + log(1-D(G(z)))` where `x~p_d` and `z~p_z`.

There are some caveats, since it needs to be implemented iteratively, D should always be slightly stronger than G so that there is always a gradient. Additionally, instead of `min_G log(1-D(G(z)))` the authors use `max_G log(D(G(z)))` as the objective for the generator.

### Dataset


### Evaluation and results


#### Preprocessing

#### Metrics


#### Results:


## Thoughts

